---
type: docs
author: ["Dataviz Team"]
title: "Statistical Testing"
thumbnail: 
description: Statistical Modeling Part 4 - What is Statistical testing? Why do we need to test? How do we perform tests?
date: "2021-04-07"
---  

import { HiOutlineLightBulb } from "react-icons/hi"  


## Introduction
Statistical (hypothesis) testing is a methodology in **frequentist inference** (often compared with bayesian inference) that used to determine whether the difference between sample and sample, sample and population is caused by sampling error or underlying difference. The significance test is one of the most commonly used methods of hypothesis testing and the most basic form of statistical inference, the basic principle of which is to make some assumptions about the characteristics of the population first, then make inference through studies of sampling and decide whether the hypothesis (assumptions made) should be rejected or accepted. Commonly used hypothesis test methods are Z-test, T-test, F-test and so on. These tests are called parametric tests because they relie on several assumptions for data related to underlying distribution and sampling variances.  


Parametric tests has the following applications:  
- test a hypothesis/assumption
- test the significance of a result
- compare two statistical models (estimate differences)
- determine relationship between variables  

The basis idea of hypothesis testing is the principle of 'rare events' - where an event is extremely unlikely to happen in an experiment, and the idea of contradiction. That is, in order to test whether a hypothesis $H_{0}$ is correct, first assume that the hypothesis is correct, and then make a decision to accept or reject the hypothesis $H_{0}$ according to the sample. If the observed value of the sample is classifield as a rare event, the hypothesis $H_{0}$ should be rejected, otherwise the hypothesis should be accepted. In all cases, there will be also a hypothesis $H_{1}$ called the alternative hypothesis, which is effectively the opposite of our hypothesis $H_{0}$.  

Performing a test usually consists the following steps:
1. Set the test hypothesis (or null hypothesis) and denote it as $H_{0}$, so the alternative hypothesis will be $H_{1}$.
2. Set the test level (or the significance level) $\alpha$, usually taken as 0.05 or 0.01. The value represent the probability that the null hypothesis being incorrectly rejected, given that the null hypothesis is true.
3. Choose a suitable statistical test.
4. Based on the size of the statistic and the p-value obtained from the test, determine whether to accept or reject the null hypothesis. If the p-value (probability of the observed value being as a rare event) is less than $\alpha$ then we can reject $H_{0}$. Otherwise in the case of $p > \alpha$ we accept $H_{0}$. 


### Sidenote
The major difference between the **frequentist inference** and **bayesian inference** (both are type of [statistics inference](https://www.bristol.ac.uk/medical-school/media/rms/red/4_ideas_of_statistical_inference.html)) lies in how they interpret the parameter space (i.e. all possible values of parameters we're trying to estimate). The frequentist approach considering that there is are unique true constant parameters, the observation data is generated under these parameters. Since it is not known which values the parameters are, the maximum likelihood estimation (MLE) and confidence interval are introduced to estimate parameters in the parameter space. Whilst the bayesian approach considered that the parameter itself has a probability distribution, thus parameters might not be unique. Each value in the parameter space has some probability to be the parameter used by the real model so prior distribution (prior distribution) and posterior distribution (posterior distribution) are introduced to find out the probability of each parameter value in the parameter space.  

---

When you use test functions in R you will often see *confidence interval* included in your results with certain percentage. A confidence interval estimates a range of possible values which is very likely to include population mean. If you look closely at the formula of the confidence interval then you will see it calculates variations around the sample mean. A common misunderstanding of confidence interval is to interpret it as the probability of the population mean will fall within the interval. However, the actual meaning of the percentage is the probability of the interval produced will contain the population mean. For example, if we have a 99% confidence interval then we are confident that we will see the population mean fall within the interval in 99 out of 100 times of construction of confidence intervals. Whereas the misunderstanding put confidence on a particular confidence interval.  
  

## Prerequisites

<div className="bg-white shadow-md p-3 md:p-5 rounded-3xl text-base text-black border-1 border-gray-50">
  <div>
    <span className="py-1 px-2 bg-shefYellow text-black hover:bg-yellow-400 transition duration-300 cursor-pointer text-base font-semibold rounded-md mr-3">Required</span> 
    <p className="mt-2">None</p>
  </div>
  <div className="mt-6">
    <span className="py-1 px-2 text-white bg-shefGreen hover:bg-green-700 transition duration-300 cursor-pointer text-base font-semibold rounded-md mr-3">Recommended</span>
    <p className="mt-2">Previous chapters in this learning path, F distribution, Student's t distribution</p>
  </div>
  <div className="mt-6">
    <span className="py-1 px-2 text-white bg-shefBlue hover:bg-blue-900 transition duration-300 cursor-pointer text-base font-semibold rounded-md mr-3">Optional</span>
    <p className="mb-0 mt-2">None</p>
  </div>
</div>  

<br/>  


## T-test
The T-test is a hypothesis test that investigate the significance difference between two groups with the following assumptions:  
- data are independent and follows normal distribution
- data are ordinal or continuous  
- homogeneity of variance (meaning variances are equal among independent samples), although the [Welch's t-test](https://en.wikipedia.org/wiki/Welch%27s_t-test) doesn't require this  

The test calculates the t-value according to [Student's t distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) and if the t-value close to zero then it suggests there isn't a significant difference between two groups. There are three types of T-test:  
1. **One-sample t-test** - use for comparing a single group against a specific value.
2. **Two-sample t-test** - use for comparing groups sampled from two different population  
3. **Paired t-test** - comparing groups sampled from the same population  

In addition to three types of T-test we have mentioned, you might also need to consider whether to use a **one-tailed test** or a **two-tailed test**. The t-distribution looks similar to the normal distribution and it has two tails, and in most of the t-test we use two-tailed tests by default and test both the negative and positive differences. For example, say we want to test whether the population mean equals to 30, in this case a two-tailed test is used. If we were interested in the case when the population mean greater than or equal to 30, then a one-tailed test would be more suitable as we are not interested in the other side (the population mean is less than 30).  

Example R code:  

```r
# Two-tailed t-test (unpaired)
t.test(
  treatment1, 
  treatment2, 
  alternative = "two.sided", 
  var.equal = FALSE
)
```

![Two-tailed t-test (unpaired)](1t-test.png)

[Click here](https://www.sthda.com/english/wiki/t-test-formula) to see t-test formula.

## F-test
F-test is a test that the statistical value (or f value) obeys the [F-distribution](https://en.wikipedia.org/wiki/F-distribution) (this is the sampling distribution for ratio of variances) under the null hypothesis $H_{0}$. It is usually used to analyse a statistical model that uses more than one parameter to determine whether all or part of the parameters in the model are suitable for estimating the population. In short, the F-test compares nested models fit to the same dataset by comparing variances of models. The assumptions for data will be similar to the one in T-test:
- data are normally distributed
- data are independent from each other  
- sum of squares follows $\chi^{2}$ distributions
  
Example R code:  

```r
var.test(
  age20, 
  age25,
  ratio = 1,
  conf.level = 0.95,
  alternative = "two.sided"
)
```

![F-test](2f-test.png)


## Linear models



T-Test
ANOVA (f distribution)
Regression







Frequentist inference is the process of determining properties of an underlying distribution via the observation of data. 


## Recommended reading
[Seeing Theory - Frequentist Inference](https://seeing-theory.brown.edu/frequentist-inference/index.html#section1)  
[MIT Lecture note - Frequentist Statistics](https://www-math.mit.edu/~dav/05.dir/class17-slides-all.pdf)  

