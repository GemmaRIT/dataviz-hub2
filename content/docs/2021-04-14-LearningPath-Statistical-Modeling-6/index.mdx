---
type: docs
author: ["Dataviz Team", "Jean Russell"]
title: "Models and Distributions"
thumbnail: 
description: Statistical Modeling Part 6 - Models and Distributions
date: "2021-04-14"
---  


The difference between statistical models and other mathematical models is that statistical models are non-deterministic. Therefore, in the statistical model some variables do not have specific values either because they are random or due to there are deterministic elements that are unknown to us. In both cases, we modeling these variables using probability distributions that we think that could have generated the population and formulating statistical models around it. In general, a statistical model usually considered as a family of probability distributions where the parameters are unkown and we are interested using data to estimate these parameters. To give you a more clearer image, let's consider a simple regression model looks something like the following:  

$$
y_{i} = \alpha + \beta_{1}x_{i} + \epsilon_{i}
$$

If we assume $\epsilon_{i} \sim N(0, \sigma^{2})$ or in another word $\epsilon_{i}$ is normally distributed with mean $0$ and variance $\sigma^{2}$, then the random variable $Y$ (a set of explanatory variables) is distributed according to a Gaussian distribution with mean $\alpha + \beta_{1}X$ and variance $\sigma^{2}$. Then we would need to estimate these parameters using the [Ordinary Least Squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) technique. Of course, if you are using the `lm()` function in R then estimation of parameters is done automatically. Note that the estimation is not always unique, therefore these combinations of parameters allow distribution in different shapes which are all in a same family. 

![Beta distribution](beta.png)  
*A family of beta distribution with two shape parameters*  


The major limitation of statistical models is that the parameters are usually specified in such a way that there are linear relationships only involved. By linear they means $y=f(g(x_{1}, x_{2}, ..., x_{n}))$ where $g(x_{1}, x_{2}, ..., x_{n})$ is a linear combination of $x_{1}, x_{2}, ..., x_{n}$; where $x_{1}, x_{2}, ..., x_{n}$ are the elements are associated with the parameters in the distribution. Nonlinear modelling does exist. It is neither well developed nor properly understood.

So in statistical modelling we are estimating/modelling the parameters of the distribution. For a poisson distribution we are actually modelling the mean, for the bernoulli distribution the probability of a true value, for a Guassian distribution both the mean and standard deviation (although the standard deviation generally modelled as a constant). In previous materials we generally assume the homogenity of variance but this might not be case for some data. In this case the model (also modeling variance) will become nonlinear unless the mean is then held constant.  


As we have mentioned before, all statistical models are idealised. There isn't a probability distribution that will make a perfect approximation to our data. In fact, the relationship between change in mean and change in variance for any model is far more important in large datasets than the actual distribution of individual data points. Nevertheless, different distributions have different relationships between mean and variance, and sometimes this relationship can be seen in a mean versus variance graph.  

![Mean vs Variance](meanVariance.png)

Graphs shown in above figure shows the sample mean versus sample variance graphs for Gaussian, Binomial, Poisson and Gamma distribution with a thousands random generated samples. The line of best fit for each subgraph is also an indication of the relationship:
- Normal - points scattered randomly and the horizontal line suggesting there isn't a obvious relation between the mean and variance
- Binomial - in a binomial distribution the sample mean is always greater than the variance because variance $=$ $q$ $\times$ mean where $q$ is the probability of failure (which is less than 1). In our graph most of the mean is greater than variance
- Poisson - theoretically the mean should equal to variance, and this is also the case for our line of best fit. There are certainly some outliers but the confidence interval (shaded area) remains narrow indicating high confidence level
- Gamma - we generally have variance $=$ $\theta$ $\times$ mean where $\theta$ is the scale parameter (or the inverse of rate parameter $\beta$). In this case our $\theta$ is $\frac{5}{3}$ (or 1.66666667), and we can see the line follows this equation




normal - independent
poisson - equal
chi-square - variance = 2 mean
binomial - mean > variance / vairance = q*mean




Samples, variable residuals. See difference between the predicted value and the actual value
Plots


If not satisfies the homogenity of variance, then use other distributions

If still no good go non-parametric





